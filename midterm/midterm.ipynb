{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mid-term instructions\n",
    "\n",
    "This assignment is to help you build several databases for your major research projects. These codes should give you a great amount of ideas how to combine, clean common financial databases for analysis.\n",
    "\n",
    "We will be looking at six databases: accounting data from annual reports, stock returns data from stock exchanges, analyst reports, CEO characteristics from proxy statements, and board data from proxy statements, firm's patent grants from patent office\n",
    "\n",
    "To make these databases manageable, I will be using 2007 to 2017, 10 years US data for illustration. For your research project, it is more than enough.\n",
    "\n",
    "These data are all available via WRDS, feel free to download them if you need more of them. I will point out where to download. For our convinience, I downloaded them already and convert them to a binary format called parquet. It helps pandas to more quickly read in the data and enable us to select columns before read the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to run below code to install some additional libraries, comment it out if you have installed them\n",
    "!pip install fastparquet pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can read fundamental_annual data by using pd.read_parquet\n",
    "funda = pd.read_parquet('https://mfin6210.s3.amazonaws.com/fundamental_annual.pq',\n",
    "                        columns=['gvkey','cusip','permno','fyear','datadate','at','xrd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fundamental data has over 900 columns for an array of accounting variables you can look at the\n",
    "variable definition: https://wrds-web.wharton.upenn.edu/wrds/ds/compd/funda/index.cfm?navId=83\n",
    "\n",
    "I manage to only read four columns: permno (firm id), fyear (fiscal year), at(total asset), xrd (R&D expenses)\n",
    "by issuing columns= parameter.\n",
    "\n",
    "Of course you can add more variables if you would like. Look at the documentation to see what's in there and grab what you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is a panel. For illustration purpose, I will use accounting data to serve as the basis\n",
    "# And merge other datasets to this one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, I read stock return data. It is monthly returns for each firm each month. For variable descriptions:\n",
    "\n",
    "https://wrds-web.wharton.upenn.edu/wrds/ds/crsp/stock_a/msf.cfm?navId=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = pd.read_parquet('https://mfin6210.s3.amazonaws.com/stock_return.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert month returns to annual returns as measures of the returns\n",
    "# and calculate standard deviations of monthly returns for each year as measures of the risk\n",
    "\n",
    "# HINT: create a new dataframe called std_ret that calculates std of returns by grouping by the \n",
    "# dataframe by permno and fyear\n",
    "# and create a new dataframe called are aret that\n",
    "# calcualtes annual ret by using product of 1+ret by grouping by permno and date\n",
    "# finally, combining (merging/joining) returns and risk to a dataframe called stock_return\n",
    "\n",
    "# Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running next cell will reveal the solution, please work on it first \n",
    "# and only peek the solution if you have stucked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We left merge to accounting data. We keep everything on the left because \n",
    "# accounting data is our basis, if you do inner join, we gonna lose more and more observation as we \n",
    "# joining more datasets. So for completeness, we left merging datasets to our basis dataset\n",
    "# and deal with missing values later\n",
    "df = funda.merge(stock_return,how='left') \n",
    "# the common columns to merge on is permno and fyear,\n",
    "# so I just omit the on= parameter here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read executive characteristics and compensation dataset\n",
    "\n",
    "https://wrds-web.wharton.upenn.edu/wrds/ds/comp/execcomp/anncomp/index.cfm?navId=72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only read a few columns for illustration, they are gvkey, a firm identifier, fyear, tdc1 is total compensation, and \n",
    "# becameceo is the date CEO took the role.\n",
    "executive = pd.read_parquet('https://mfin6210.s3.amazonaws.com/exec_chars.pq',\n",
    "                            columns=['gvkey','fyear','tdc1','ceoann','becameceo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executive chars data lists top 5 executives in the company, suppose we only need CEO data\n",
    "# write code to subset the rows where ceoann='CEO', save this subset to dataframe \"ceos\",\n",
    "# After keep only CEOs, drop duplicates at gvkey, fyear level to form a firm-year panel\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge CEO characteristics back to df\n",
    "df = df.merge(ceos,how='left') # this time, the merging key is gvkey and fyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we merge director information from ISS (also called riskmetrics)\n",
    "\n",
    "https://wrds-web.wharton.upenn.edu/wrds/ds/riskmetrics/rmdirectors/index.cfm?navId=245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row represents an individual director for a company (cusip) for each year (need to derive from meetingdate)\n",
    "directors = pd.read_parquet('https://mfin6210.s3.amazonaws.com/directors.pq',\n",
    "                            columns=['cusip','meetingdate','director_detail_id','classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to convert meetingdate to fiscal year, if the month < 7, then it is the calendar year - 1\n",
    "# if the month >= 7, it is the calendar year\n",
    "# HINT: first convert meetingdate to pandas' datetime format\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these data comes from proxy statements, sometime company switch their reporting schedule so we may have duplicated reporting\n",
    "# in some years, but these cases are rare. For our purpose, we need to make sure certain director only appear once in a year\n",
    "# Therefore, we drop duplicated directors in each firm-year\n",
    "# (this is cruel way of dealing with duplicates, but since the impact is small, we just force drop the duplicates)\n",
    "directors = directors.drop_duplicates(['cusip','fyear','director_detail_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate an indicator to indicate the director is an independent director\n",
    "directors['independence'] = directors['classification'].str.contains('I')*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate two measures:\n",
    "1. Board size\n",
    "2. the fraction of independent directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to count unique directors in each firm-year as the measure of board size\n",
    "# write code to calculate the fraction of independent directors\n",
    "# Your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independence = (independence / boardsize).rename('independence') # calculate the fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = pd.concat([boardsize,independence],axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(board,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will merge the analyst report dataset, which contains analyst's forecast EPS for certain company. We will create a few measures there:\n",
    "\n",
    "https://wrds-web.wharton.upenn.edu/wrds/ds/ibes/det/index.cfm?navId=223\n",
    "\n",
    "1. Analyst coverage: Number of analysts are there to predict the company's EPS\n",
    "1. Analyst forecast volatility: Std of analyst's forecasts, a measure of firm's information opacity\n",
    "1. Analyst's forecast level: The median forecast EPS from all analyst for that firm-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, to make the data managable, I only keep firm id (cusip), forecasting date, actual value of forecast and analyst code\n",
    "# The whole data file is very big, please only select columns that you need\n",
    "analyst = pd.read_parquet('https://mfin6210.s3.amazonaws.com/analyst_eps.pq',\n",
    "                          columns=['cusip','fpedats','value','analys'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to calculate coverage, forecast volatility and forecast level, create three dataframes:\n",
    "# coverage, analyst_volatility, analyst_median\n",
    "# HINT: group by cusip and fpedats\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forecast is made for every reporting period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst = pd.concat([coverage,analyst_median,analyst_volatility],axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for us to merge the analyst data back to df, we need to do two things:\n",
    "# write code to:\n",
    "# 1. the cusip in analyst dataset is only first 8 digits, so we need to convert df's cusip to the first 8 digits from 9 digits\n",
    "# 2. rename analyst's fpedats to datadate, so we can match on column's name for pandas to merge\n",
    "# you code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(analyst,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will merge number of patents granted for each firm-year as a measure of innovation\n",
    "\n",
    "I have already cleaned the data for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_count = pd.read_parquet('https://mfin6210.s3.amazonaws.com/patents.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(pat_count,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "\n",
    "if len(list(df))==18:\n",
    "    print('congrants! You have successfully completed the exercise')\n",
    "else:\n",
    "    assert len(list(df))==18, 'Sorry, you did not complete the exercise'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
