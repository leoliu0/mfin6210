{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mid-term Exam Instructions\n",
    "\n",
    "This assignment is designed to help you build several databases for your major research projects. These codes should give you a detailed indication on how to combine and clean common financial databases for analysis.\n",
    "\n",
    "We will be looking at six databases: \n",
    "Accounting data from annual reports, \n",
    "Stock returns data from stock exchanges, \n",
    "Analyst reports, \n",
    "CEO characteristics from proxy statements, \n",
    "Board data from proxy statements, and \n",
    "Firm's patent grants from patent office\n",
    "\n",
    "To make these databases manageable, I will be using data from 2007 to 2017, 10 years US data, for illustration. For your research project, it is more than enough.\n",
    "\n",
    "These datas are all available via WRDS, feel free to download them if you need more of them. I will point out where to download. For our convenience, I have downloaded them already and converted them into a sqlite db file. It helps pandas read in the data more quickly and enables us to select the columns before reading the entire dataset.\n",
    "\n",
    "\n",
    "Tips before you begin:\n",
    "\n",
    "1) Go back through the videos from week 1, week 2 and week 3 if you get stuck on anything. All of the tools required to complete this task have been already taught, you should be able to recognise which tools to use. If you cannot remember how to perform a certain task in pandas, go through the videos until you find the solution, or search it up on google.\n",
    "\n",
    "2) Google is your best friend! Search up anything that you are still stuck with, and make sure that you are able to fully understand and explain what each function does that allows you to perform your required task.\n",
    "\n",
    "3) I want you to demonstrate your knowledge and understanding of python. Put in comments in your code to explain what you have done in each step, present it clearly and imagine you are showing this code to someone with absolutely no knowledge of python. You want them to understand what it is you have done.\n",
    "\n",
    "4) There will be cells that show the solutions for these tasks. Only use these if you are completely stuck. Avoid directly copying the code from the solution into your answer. You should try to run the code first, and continue to try and try again until you get the code to run properly. Then, I will recommend you check your code with the solutions after you are confident that your code is working.\n",
    "\n",
    "5) Good luck!\n",
    "\n",
    "Note: for any Mac users who are struggling to access Jupyter Notebook on their computer, here are the instructions to open Jupyter Notebook:\n",
    "\n",
    "1. Download the midterm.ipynb zip on GitHub\n",
    "\n",
    "2. Expand the zip on your desktop and place file on desktop\n",
    "\n",
    "3. Go into terminal\n",
    "\n",
    "4. Type pip install anaconda in terminal\n",
    "\n",
    "5. After anaconda is installed, type jupyter notebook in terminal. This will take you to a web server where you can access the Jupyter Notebook.\n",
    "\n",
    "6. Open the midterm.ipynb code from your desktop and the mfin6210-master file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to run below code to install some additional libraries, comment it out if you have installed them\n",
    "# wget is a simple library to download files\n",
    "!pip install wget\n",
    "# if this one fails, open anaconda prompt and do\n",
    "# pip install wget\n",
    "# Or,\n",
    "# conda install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download my database file, this should only run once and comment it out after done\n",
    "# you can copy that url to a browser to download it, too.\n",
    "# it's gonna take a few minutes\n",
    "if not os.path.isfile('midterm.db'):\n",
    "    wget.download('https://mfin6210.s3.amazonaws.com/midterm.db','midterm.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect('midterm.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can read fundamental_annual data by using pd.read_sql function\n",
    "funda = pd.read_sql('SELECT gvkey,cusip,permno,fyear,datadate,at,xrd FROM funda',db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamental data has over 900 columns for an array of accounting variables, you can look at the\n",
    "variable definition: https://wrds-web.wharton.upenn.edu/wrds/ds/compd/funda/index.cfm?navId=83\n",
    "\n",
    "I manage to only read four columns: permno (firm id), fyear (fiscal year), at(total asset), xrd (R&D expenses)\n",
    "\n",
    "Of course you can add more variables if you would like. Look at the documentation to see what's in there and grab what you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is a panel. For illustration purposes, I will use accounting data to serve as the basis\n",
    "# And merge all other datasets into this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to produce a histogram for the normal distribution of the assets in the firms. \n",
    "# Your first task:\n",
    "# 1. Write code to read in the log of the asset values\n",
    "# 2. Produce a histogram that produces a normal distribution. \n",
    "\n",
    "# Hints:\n",
    "# A normal distribution should be a curve that is very smooth and evenly spread out\n",
    "# When taking the log of the values, make sure to add 1 to the values by creating a new column\n",
    "# create a column called ln_at that takes the log of the asset and adds 1 to the value\n",
    "\n",
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the next cell will reveal the solution, please work on it first. \n",
    "# Do not copy directly out of the solutions because I will be able to tell whether you have given this a serious attempt\n",
    "# and only look the solution if you are absolutely stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I read stock return data. It is the monthly returns for each firm. For variable descriptions:\n",
    "\n",
    "https://wrds-web.wharton.upenn.edu/wrds/ds/crsp/stock_a/msf.cfm?navId=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = pd.read_sql('''SELECT permno, fyear, date, ret FROM stock_return\n",
    "                    WHERE ret IS NOT NULL''',db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your second task:\n",
    "# 1. Write code to convert the monthly returns into annual returns as a measure of the returns\n",
    "# 2. Calculate standard deviations of monthly returns for each year as a measure of the risk\n",
    "\n",
    "# HINT: create a new dataframe called std_ret that calculates standard deviation of returns by grouping the \n",
    "# dataframe by permno and fyear, (google up the pandas tool for standard deviation)\n",
    "# and create a new dataframe called are aret that\n",
    "# calculates annual ret by using product of 1+ret. Do this by grouping the data by permno and fyear\n",
    "# finally, combining (merging/joining) returns and risk to a dataframe called stock_return\n",
    "\n",
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once again, only use this as a last resort scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use left merge for accounting data. We keep everything on the left because \n",
    "# accounting data is our basis. If we do inner join, we will lose more and more observations as we \n",
    "# join more datasets. So, for the completeness of the data, we will left merge datasets into our base dataset\n",
    "# and deal with the missing values later\n",
    "df = funda.merge(stock_return,how='left') \n",
    "# the common columns to merge on is permno and fyear,\n",
    "# so I just omitted the on= parameter here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read executive characteristics and compensation dataset\n",
    "\n",
    "https://wrds-web.wharton.upenn.edu/wrds/ds/comp/execcomp/anncomp/index.cfm?navId=72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have only read a few columns for illustration, \n",
    "# they are: gvkey is a firm identifier, fyear (fiscal year), tdc1 is total compensation, \n",
    "# and becameceo is the date CEO took the role.\n",
    "executive = pd.read_sql('select gvkey,fyear,tdc1,ceoann,becameceo FROM exec_chars',db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your third task:\n",
    "# Executive chars data lists the top 5 executives in the company, suppose we only need the data for the CEO\n",
    "# 1. Write code to subset the rows where ceoann='CEO', save this subset to dataframe \"ceos\",\n",
    "# 2. After keeping only CEOs, drop duplicates at gvkey and fyear level to form a firm-year panel\n",
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We now merge CEO characteristics back to df\n",
    "df = df.merge(ceos,how='left') # this time, the merging keys are gvkey and fyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we merge director information from ISS (also called riskmetrics)\n",
    "\n",
    "https://wrds-web.wharton.upenn.edu/wrds/ds/riskmetrics/rmdirectors/index.cfm?navId=245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row represents an individual director for a company (cusip) for each year (need to derive from meetingdate)\n",
    "directors = pd.read_sql('''\n",
    "            SELECT cusip,meetingdate,director_detail_id,classification from directors\n",
    "            ''',db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your fourth task:\n",
    "# 1. Write code to convert meetingdate to fiscal year, if the month < 7, then it is the calendar year - 1\n",
    "# if the month >= 7, it is the calendar year\n",
    "# HINT: first convert meetingdate to pandas' datetime format (google it if you need to find out how to do this)\n",
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data comes from proxy statements, sometimes company will switch their reporting schedule \n",
    "# so we may have duplicated reporting In some years, but these cases are rare. \n",
    "# For our purpose, we need to make sure a certain director will only appear once in a year\n",
    "# Therefore, we drop duplicated directors in each firm-year\n",
    "# (this is cruel way of dealing with duplicates, but since the impact is small, we will just force drop the duplicates)\n",
    "directors = directors.drop_duplicates(['cusip','fyear','director_detail_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate an indicator to indicate the director is an independent director\n",
    "directors['independence'] = directors['classification'].str.contains('I')*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate two measures:\n",
    "1. Board size\n",
    "2. the fraction of independent directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your fifth task:\n",
    "# 1. Write code to count the number of unique directors in each firm-year as the measure of board size\n",
    "# (once again, google this if you are unsure of how to count the number of unique directors)\n",
    "# 2. Write code to calculate the fraction of independent directors\n",
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independence = (independence / boardsize).rename('independence') # calculate the fraction\n",
    "# notice I rename the pandas series to independence, this is not strictly required, but for cosmetic purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = pd.concat([boardsize,independence],axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(board,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will merge the analyst report dataset, which contains analyst's forecast EPS for a certain company. We will create a few measures there:\n",
    "\n",
    "https://wrds-web.wharton.upenn.edu/wrds/ds/ibes/det/index.cfm?navId=223\n",
    "\n",
    "1. Analyst coverage: Number of analysts existing to predict the company's EPS\n",
    "2. Analyst forecast volatility: Std of analyst's forecasts, a measure of firm's information opacity\n",
    "3. Analyst's forecast level: The median forecast EPS from all analysts for that firm-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, to make the data managable, \n",
    "# I will only keep firm id (cusip), forecasting date, actual value of forecast and analyst code\n",
    "# The whole data file is very big, please only select columns that you need\n",
    "analyst = pd.read_sql('SELECT cusip,fpedats,value,analys from analyst_eps',db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your sixth task:\n",
    "# 1. Write code to calculate coverage, forecast volatility and forecast level, create three dataframes:\n",
    "# coverage, analyst_volatility, analyst_median\n",
    "# HINT: group by cusip and fpedats (google how to calculate any of these values that you don't know how to do)\n",
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forecast is made for every reporting period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst = pd.concat([coverage,analyst_median,analyst_volatility],axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your seventh task:\n",
    "# For us to merge the analyst data back to df, we need to do two things:\n",
    "# The cusip in analyst dataset is only first 8 digits\n",
    "# Write code to:\n",
    "# 1. Convert df's cusip to the first 8 digits from 9 digits\n",
    "# 2. Rename analyst's fpedats to datadate, so we can match on column's name for pandas to merge\n",
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 7.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(analyst,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have want to have stock ownership information. WRDS has already got a nice dataset for us.\n",
    "\n",
    "https://wrds-web.wharton.upenn.edu/wrds/ds/tfn/types/s34summary/index.cfm?navId=340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "io = pd.read_sql('SELECT rdate,cusip,NumInstBlockOwners,InstOwn_HHI,InstOwn_Perc FROM inst_own',db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(io,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will merge the number of patents granted for each firm-year as a measure of their innovation\n",
    "\n",
    "I have already cleaned the data for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_count = pd.read_sql('SELECT * FROM patent',db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(pat_count,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "if len(list(df))==22:\n",
    "    print('Congratulations! You have successfully completed the exercise!')\n",
    "else:\n",
    "    print('Sorry, you did not complete the exercise')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
